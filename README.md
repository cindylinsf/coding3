# Remaking Hitchcock's 1963 film _The Birds_

üóÇÔ∏è **Project Files: [[Google Drive Files for this Project](https://drive.google.com/drive/folders/1GDVN3x3jcEExj150KVBS02H0M8h5oB02?usp=drive_link)]**

This project is inspired by Joanna Zylinska's work [_The Gift of the World (Oedipus on the Jetty)_](https://www.nonhuman.photography/perception-machine), a remake of [_La Jet√©e_](https://www.youtube.com/watch?v=DQ4jFYTTjAU).

_La Jet√©e_ is a 1962 French science fiction featurette directed by Chris Marker and associated with the Left Bank artistic movement. Constructed almost entirely from still photos, it tells the stable time loop story of a post-nuclear war experiment in time travel. It is 28 minutes long and shot in black and white. [1] To create this work, Zylinska generated the new script by feeding selected lines from _La Jet√©e_'s original script to GPT-2, which took the film in an unexpected new direction by rewriting the very masculine story of salvation present in _La Jet√©e_ as a gender-fluid polyvocal counter-apocalypse.[2] To produce the film, she combined a variety of photorealistic AI models such as GANs, Diffusion, and CLIP. [3]

![Zylinska (2023), Composite of frames from A Gift of the World (Oedipus on the Jetty), [Photofilm, 2021]. ¬© Zylinska, 2023 (Cortes√≠a de la Autora)](Zylinska-2023-Composite-of-frames-from-A-Gift-of-the-World-Oedipus-on-the-Jetty.png)
Zylinska (2023), Composite of frames from A Gift of the World (Oedipus on the Jetty), [Photofilm, 2021]. ¬© Zylinska, 2023 (Cortes√≠a de la Autora) [4]

For Zylinska, the editing process was intuitive and consisted of her own visual and corporeal responses to both the script and the GAN images, which she references as a dream-like state, where neural networks find patterns in images not so much in a logical preprogrammed way but rather by using previous data and memories as prompts for making new connections between data points and for generating new data. In this work, she finds herself dreaming _with_ but also _against_ the AI algorithm underpinning the model. The "counterdream" aspect was critical given the sociopolitical limitations of AI technology, which have been well documented by feminist and decolonial cultural critics, revealing not only gender and racial bias but also the exclusionary and unjust logic underlying many of its founding principles. Not to mention its extractivism when it comes to both human and natural resources. Zylinska was curious about what AI would do to the source material, while intentionally serving as both a dream catcher and analyst in the creation process. [5]

When I came across Zylinska's project, I thought it was interesting how GPT-2 flipped the gender narrative of the original film, which shifted the premise of the film dramatically. The abstract images created by StyleGAN was also very interesting. Human faces were re-interpreted with these kidneys-liked eyees. Inspired by this, I chose to experiment with Hitchcock's _The Birds_, a film from a similar era with a male-centric storyline lacking female perspectives, perpetuating sexist stereotypes, and depicting violence against women. I also wanted to see if any biases would be reflected in the AI-generated outcomes.

## Approach & Process

### Text Generation
üóÇÔ∏è **Project Files:** 
- [[minibpe tokenizer outputs](https://drive.google.com/drive/folders/1jCvXqMzpysmWJ-UuEMF0c4-JZHcbfewu?usp=drive_link)]
- [[Google Colab notebook 00_tokenize_script](https://drive.google.com/file/d/1pWiDtswiTl1T_0vfRmgdLPGf84bn4ucg/view?usp=drive_link)]
- [[Google Colab Notebook 01_new_script_generation](https://colab.research.google.com/drive/10OAtNJP5Rr8oz06NzSipFfAx3OBJ-5w7?authuser=1#scrollTo=GpisUfhuJli_)]
- [[Google Colab Notebook 02_new_script_genreation_by_finetuning](https://drive.google.com/file/d/1KpFxMCUrMBQ0LSmCBP6Ko_5EnITU4CkA/view?usp=sharing)]
- [[`output_scripts` folder containing scripts generated by the model](https://drive.google.com/drive/folders/1AT7PVAKPRwRQcmN2txz4ft1pUZj-kWM7?usp=drive_link)]
- [[final_script_generated_by_gpt4o](https://docs.google.com/document/d/1I_U_NPr-mMuNb9Sg9Ax7g-k9ncE_g1y8ktEya8hRLjE/edit?usp=sharing)]

ü§ñ **Model used:**
- [[Andrej Karpathy's minibpe repo](https://github.com/karpathy/minbpe)]
- NanoGPT from Class 5 Notebooks

For this part of the project, I adapted the Class 5 notebooks to create and prepare a new dataset by using [[the original 1962 film script] (http://www.script-o-rama.com/movie_scripts/b/the-birds-script-screenplay.html)] and generate new scripts by adapting Class 5 notebooks. 

A separate tokenization was not needed with the Class 5 notebooks, but since I was curious about the tokenization process, I tokenized the film script using Andrej Karapthy's minibpe repo mentioned in the class notebook. The results and the Google Colab notebook `00_tokenize_script` are referenced above under `üóÇÔ∏è Project Files`.

I first mimicked Zylinska's process by setting the start seed for the model by using the first line of the original script. Then I realized that the model used it repeatedly for each new scene, so I removed the start seed. The first two generated outputs with the start seed are in the `output_scripts` folder as `output_GPT2_v1` and `output_GPT2_v2`.

After that, I let the GPT2 model generate freely without the start seed for four additional times, the results are collected under `output_GPT2_v3`, `output_GPT2_v4`, `output_GPT2_v5`, and `output_GPT2_v6`. I then combined these four outputs and fed them through ChatGPT 4o:

> The PDF contains a few different versions of a short film draft. Please clean up this short film draft and rewrite it to make it coherent for today's audience.

After its initial generation, I prompted it further:

> This is very intriguing, can you keep writing?

The final script output is recorded in the `final_script_generated_by_gpt4o` Google Doc referenced under `üóÇÔ∏è Project Files` above. 

### StyleGAN-T Generation (FAILED)
üóÇÔ∏è **Project File:**
- [[Google Colab Notebook 03](https://colab.research.google.com/drive/1ntrxQQxkczMidhcOKD5hhbnj5KnA4sod?usp=sharing)]

ü§ñ **Model used:**
- [[StyleGAN-T model](https://github.com/autonomousvision/stylegan-t/blob/main/README.md)]

Here, I tried to run the generated script through the StyleGAN-T model, which combines transformers with StyleGAN. This means that I could omit CLIP text encoding mapping to StyleGAN's latent space. However, I was not able to overcome dependencies and package conflicts in the StyleGAN-T model. After 2+ hours of debugging and trying various ways of installing different packages, I decided to try other methods. 

### CLIP Text Encoding
üóÇÔ∏è **Project Files:**
- [[Part 1 of Google Colab Notebook 04](https://colab.research.google.com/drive/1Emp0_7DMXqtKxo7338TxcLxxQTeVtXu8?usp=sharing)]

ü§ñ **Model used:**
- [[CLIP](https://github.com/openai/CLIP)]

Here, I used OpenAI's CLIP model. The CLIP text encoding was successful and I saved the output in the `text_embeddings` variable. 

### Mapping to StyleGan 2 Latent Space (FAILED)
üóÇÔ∏è **Project Files:**
- [[Part 2 of Google Colab Notebook 04](https://colab.research.google.com/drive/1Emp0_7DMXqtKxo7338TxcLxxQTeVtXu8?usp=sharing)]

ü§ñ **Model used:**
- [[StyleGAN 2](https://github.com/NVlabs/stylegan2.git)]

The CLIP encoding was created successfully in the previous step, however, I ran into issues when mapping it to StyleGAN2's latent space.

ChatGPT helped me write a Mapper function to map out the latent space, but I had issues defining the latent vectors using the code from the StyleGAN2 Read Me file. I could not install a few of the modules required by the model. After trying to re-install and install different versions with no results, I decided to try stable diffusion.

### Stable Diffusion (FAILED)
üóÇÔ∏è **Project Files:**
- [[Google Colab Notebook 05](https://colab.research.google.com/drive/1ieXwFiM5GT38Z7mD3_2DgRkq62pHADGd?usp=sharing)]

ü§ñ **Models used:**
- Class 7 notebooks

I decided to try stable diffusion since it is a generative text-to-image model. I had issues adapting the Class 7 notebook in the Google Colab environment, where it could not locate the CLIP checkpoint in the `python3.10/dist-packages/torch/serialization.py` file. I was not able to debug this successfully with my limited Python skills.

### CLIP-Guided Diffusion
üóÇÔ∏è **Project Files:**
- [[Google Colab Notebook 06](https://colab.research.google.com/drive/1A3iLLQFrVtWMwtyHAbFW05ysFHuS2wNf?usp=sharing)]

ü§ñ **Model used:**
- [[CLIP-Guided Diffusion by EleutherAI](https://www.eleuther.ai/artifacts/clip-guided-diffusion)]

I had a lot of issues locating the latent vector to map my text encoding from CLIP to StyleGAN2's latent space due to dependencies being updated and becoming conflicted with each other. After researching, I found the CLIP-Guided Diffusion model by EleutherAI to successfully generate images with text prompts. 

This particular model is adapted from [Katherine Crowson(https://twitter.com/RiversHaveWings)]'s [work (https://github.com/crowsonkb/clip-guided-diffusion)]. It uses [OpenAI's 256x256 unconditional ImageNet diffusion model (https://github.com/openai/guided-diffusion)] together with [CLIP (https://github.com/openai/CLIP)] to connect text prompts with images. 

Each image takes about 9 minutes to generate. In the interest of on-time delivery of this project, I only generated the first scene (first 10 prompts) of the script.

Full list of prompts: https://docs.google.com/document/d/1CxwlhFuR-utL1xmJVt2GeadrJO1squYzX-DkUVv6Cv0/edit?usp=sharing 

### Stable Diffusion #2
üóÇÔ∏è **Project Files:**
- [[Google Colab Notebook 07](https://colab.research.google.com/drive/1LK7CT87Ui-YsE0CG966QbcX2DffzVIZf?usp=sharing)]

ü§ñ **Models used:**
- [[StabilityAI's SDXL Turbo model on Hugging Face](https://huggingface.co/stabilityai/sdxl-turbo)]

### Image Interpolation
üóÇÔ∏è **Project Files:**


## Results & Evaluation

# Bibliography
1. La Jet√©e. (2024, June 6). In Wikipedia. https://en.wikipedia.org/wiki/La_Jet%C3%A9e
2. Talking to Joanna Zylinska. Artificial intelligence in artistic creation., Jose Vertedor, December 2023 - Um√°tica. Revista sobre Creaci√≥n y An√°lisis de la Imagen.
3. The Perception Machine, Joanna Zylinska, 2023, page 136 - Massachusetts Institute of Technology.
4. Vertedor-Romero, J.A.. (2023). Editorial. AI-driven art: la inteligencia artificial en el arte y el dise√±o. UM√ÅTICA. Revista sobre Creaci√≥n y An√°lisis de la Imagen. 9-20. 10.24310/umatica.2023.v5i6.18315. 
5. The Perception Machine, Joanna Zylinska, 2023, page 139-140 - Massachusetts Institute of Technology.
